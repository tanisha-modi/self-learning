{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "AIM : create a self learning model, that has the ability to detect the a specific object on which the model is trained on field.\n",
        "\n",
        "\n",
        "STEPS :\n",
        "\n",
        "step 1 : placing the tag just above the object and take photos.(almost 100-120 photos)\n",
        "\n",
        "step 2 : perform tag detection on it and save the images on which the tag is detected in different folder.\n",
        "\n",
        "step 3 : create a bounding box just below the tag and generate the txt file for the object (according to the size of object).\n",
        "\n",
        "step 4 : augment the images along with the txt files.\n",
        "\n",
        "step 5 : train the model for object."
      ],
      "metadata": {
        "id": "p86_Hb24kCu0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNKxiDe9jDjK"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x6i56qMEvKc",
        "outputId": "86b5ef9f-7318-474c-8e75-9f31159e0533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 29.3/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1sgZBvWipDL",
        "outputId": "1742b5c8-2d74-4de6-a720-f49399523ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ultralytics --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F59KaZrBi2g4",
        "outputId": "1352d0b8-5866-4a42-a247-83f8423770c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Ultralytics settings reset to default values. This may be due to a possible problem with your settings or a recent ultralytics package update. \n",
            "View settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.yaml'\n",
            "Update settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "WARNING ⚠️ argument '--version' does not require leading dashes '--', updating to 'version'.\n",
            "8.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgD3wS-yQBTh"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VB9NdifjHZv"
      },
      "source": [
        "# Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ojxtR7KU9yN",
        "outputId": "315becd8-8e0a-471c-bf3f-70cbf9a1c63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM61PSRqjNUE"
      },
      "source": [
        "# Set Path\n",
        "\n",
        "change directory to the folder in which we want to save the predict folder for tag detection on dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrLOh4oiUy9W",
        "outputId": "3a0e7716-752a-4d63-f7b9-e4406fe7a2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Last/Tag\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Last/Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlVi1arsjR2n"
      },
      "source": [
        "# tag detection on dataset\n",
        "\n",
        "predicting the tag\n",
        "\n",
        "model - the model for tag detection\n",
        "\n",
        "source - dataset (100 clicked images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxG8pguZUgXw",
        "outputId": "d2bab8b4-4e0c-432c-f6e0-4deb8aee2667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-10 09:23:29.425723: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-10 09:23:29.425857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-10 09:23:29.428466: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-10 09:23:31.719129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "image 1/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141636.jpg: 640x480 1 tag, 699.7ms\n",
            "image 2/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141643.jpg: 640x480 1 tag, 507.2ms\n",
            "image 3/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141829.jpg: 640x480 1 tag, 508.9ms\n",
            "image 4/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141830.jpg: 640x480 1 tag, 863.9ms\n",
            "image 5/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141859.jpg: 640x480 1 tag, 1058.8ms\n",
            "image 6/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141936.jpg: 640x480 1 tag, 561.1ms\n",
            "image 7/34 /content/drive/MyDrive/Sharpener/IMG_20240402_141940.jpg: 640x480 1 tag, 483.6ms\n",
            "image 8/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142012.jpg: 640x480 1 tag, 528.4ms\n",
            "image 9/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142049.jpg: 640x480 1 tag, 487.9ms\n",
            "image 10/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142055.jpg: 640x480 1 tag, 576.2ms\n",
            "image 11/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142117.jpg: 640x480 1 tag, 818.9ms\n",
            "image 12/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142136.jpg: 640x480 1 tag, 576.6ms\n",
            "image 13/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142228.jpg: 640x480 1 tag, 494.7ms\n",
            "image 14/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142247.jpg: 640x480 1 tag, 540.6ms\n",
            "image 15/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142249.jpg: 640x480 1 tag, 513.7ms\n",
            "image 16/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142412.jpg: 640x480 2 tags, 543.3ms\n",
            "image 17/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142414.jpg: 640x480 2 tags, 726.2ms\n",
            "image 18/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142507.jpg: 640x480 2 tags, 855.5ms\n",
            "image 19/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142554.jpg: 640x480 1 tag, 545.9ms\n",
            "image 20/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142624.jpg: 640x480 1 tag, 539.1ms\n",
            "image 21/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142815.jpg: 640x480 1 tag, 568.0ms\n",
            "image 22/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142828.jpg: 640x480 1 tag, 557.2ms\n",
            "image 23/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142833.jpg: 640x480 1 tag, 506.4ms\n",
            "image 24/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142852.jpg: 640x480 1 tag, 822.9ms\n",
            "image 25/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142852__01.jpg: 640x480 1 tag, 853.4ms\n",
            "image 26/34 /content/drive/MyDrive/Sharpener/IMG_20240402_142930.jpg: 640x480 1 tag, 478.8ms\n",
            "image 27/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143009.jpg: 640x480 1 tag, 475.2ms\n",
            "image 28/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143020.jpg: 640x480 1 tag, 479.3ms\n",
            "image 29/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143042.jpg: 640x480 1 tag, 512.5ms\n",
            "image 30/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143158.jpg: 640x480 1 tag, 497.3ms\n",
            "image 31/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143243.jpg: 640x480 1 tag, 834.9ms\n",
            "image 32/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143323.jpg: 640x480 1 tag, 673.1ms\n",
            "image 33/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143333.jpg: 640x480 1 tag, 490.4ms\n",
            "image 34/34 /content/drive/MyDrive/Sharpener/IMG_20240402_143343.jpg: 640x480 1 tag, 476.4ms\n",
            "Speed: 1.4ms pre-process, 607.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect mode=predict model=\"/content/drive/MyDrive/self-learn/runs/detect/train10/weights/best.pt\" conf=0.25 source=\"/content/drive/MyDrive/Sharpener\" save=true plots=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDrsppetjZSn"
      },
      "source": [
        "# extract imformation from result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPaFLFsPVI1Y"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "model = YOLO( \"/content/drive/Shareddrives/x/tag_detect/train10/weights/best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowing the size of images"
      ],
      "metadata": {
        "id": "RMQbvPTfluiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Path to the folder containing images\n",
        "folder_path = \"/content/drive/MyDrive/Sharpener\"\n",
        "\n",
        "# List all files in the folder\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# Filter out only the image files (you may need to adjust this depending on the types of images you have)\n",
        "image_files = [f for f in files if f.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\"))]\n",
        "\n",
        "for image_file in image_files:\n",
        "    # Construct the full path to the image\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Get the size of the image\n",
        "    width, height = image.size\n",
        "\n",
        "    print(f\"Image width: {width}, Image height: {height}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa7JTS14Glmn",
        "outputId": "e0a17a73-1ad0-4b6b-888a-db497ec02867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image width: 3000, Image height: 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extracting results and saving it to an list named \"n\"**\n",
        "\n",
        "**Masking of tag - reducing overfitting induced due to presence of tag in every image**"
      ],
      "metadata": {
        "id": "EFBz4-gNoBED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggoqno7JXj_k",
        "outputId": "16ffa501-93af-42a6-9bda-0d8db794cdb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# for masking tags in images\n",
        "def mask_objects(image, bboxes):\n",
        "    mask = np.zeros_like(image)\n",
        "    for bbox in bboxes:\n",
        "        x, y, w, h = bbox\n",
        "        bbox_mask = np.zeros_like(image)\n",
        "        bbox_mask[0:y+int(h/2), 0:width] = 1\n",
        "        mask = np.maximum(mask, bbox_mask)\n",
        "    masked_image = np.where(mask == 1, (128, 128, 128), image)\n",
        "\n",
        "    # Save the masked image\n",
        "    filename = os.path.basename(image_path)\n",
        "    output_path = os.path.join(output_folder, filename)\n",
        "    cv2.imwrite(output_path, masked_image)\n",
        "# -------------------------------------------------------\n",
        "\n",
        "\n",
        "# Path to the folder containing images\n",
        "folder_path = \"/content/drive/MyDrive/Sharpener\"\n",
        "output_folder = \"/content/drive/MyDrive/final/code_modify\"\n",
        "\n",
        "# List all files in the folder\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "# Filter out only the image files (you may need to adjust this depending on the types of images you have)\n",
        "image_files = [f for f in files if f.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\"))]\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# randomly suffles the images, so that the random images are going to masked\n",
        "random.shuffle(image_files)   # Shuffle the images\n",
        "num_images = len(image_files)  # number of images on which the tag is detected\n",
        "# ---------------------------------------------------\n",
        "\n",
        "\n",
        "# Define a list to store all tensor values\n",
        "n = []\n",
        "cou = 0\n",
        "# Iterate over each image file\n",
        "for image_file in image_files:\n",
        "    # Construct the full path to the image\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "\n",
        "    # Apply your model to the image\n",
        "    results = model(image_path)\n",
        "\n",
        "    # Iterate over each result for the image\n",
        "    for result in results:\n",
        "        # Extract tensor values and append to the list\n",
        "        bbox = result.boxes.xywh.tolist()\n",
        "        bbox = [[int(element) for element in sublist] for sublist in bbox]\n",
        "        predict_class = result.boxes.cls.tolist()\n",
        "        image_name = os.path.splitext(image_file)[0]\n",
        "\n",
        "        # if the image contains tag and it is detected\n",
        "        if(len(bbox) > 0):\n",
        "            # saving the image(in which tag is detected) in separate folder\n",
        "            img = cv2.imread(image_path)\n",
        "            filename = os.path.basename(image_path)\n",
        "            # cv2.imwrite(save_path, img)\n",
        "\n",
        "            destination_path = os.path.join(output_folder, filename)\n",
        "            if(cou < num_images/2):\n",
        "              mask_objects(img, bbox)\n",
        "            else:\n",
        "              shutil.copyfile(image_path, destination_path)\n",
        "\n",
        "            # saving the results in a list named \"n\"\n",
        "            n.append({\"image_name\": image_name, \"bbox\": bbox, \"predict_class\": predict_class})\n",
        "            cou += 1\n",
        "\n",
        "\n",
        "# Print all stored tensor values\n",
        "# for tensor_value in n:\n",
        "#     print(tensor_value)\n",
        "#     cou = cou + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypnmNTYVhA4f"
      },
      "source": [
        "# txt file generation\n",
        "The txt file annotation format is supported by yolov8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-qe8fq4FfhC",
        "outputId": "cfad9c52-6a63-4f75-c2fd-6c4eb89d8d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Last/data\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142833.txt/n\n",
            "1\n",
            "0\n",
            "1545 2001 643 500\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142228.txt/n\n",
            "1\n",
            "0\n",
            "1573 1893 720 596\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143343.txt/n\n",
            "1\n",
            "0\n",
            "669 2052 647 568\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142249.txt/n\n",
            "1\n",
            "0\n",
            "1459 1971 629 567\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142012.txt/n\n",
            "1\n",
            "0\n",
            "1548 2186 713 730\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141643.txt/n\n",
            "1\n",
            "0\n",
            "1744 2412 863 761\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141830.txt/n\n",
            "1\n",
            "0\n",
            "1593 1421 840 787\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142852__01.txt/n\n",
            "1\n",
            "0\n",
            "1609 1340 760 620\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142624.txt/n\n",
            "1\n",
            "0\n",
            "1510 2031 728 655\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143323.txt/n\n",
            "1\n",
            "0\n",
            "1504 1920 724 647\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142852.txt/n\n",
            "1\n",
            "0\n",
            "1562 1552 776 622\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142247.txt/n\n",
            "1\n",
            "0\n",
            "1354 2184 678 489\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141940.txt/n\n",
            "1\n",
            "0\n",
            "1653 2719 680 584\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141936.txt/n\n",
            "1\n",
            "0\n",
            "1424 1672 725 664\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142412.txt/n\n",
            "2\n",
            "0\n",
            "2121 2153 689 573\n",
            "1\n",
            "802 1431 656 550\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142507.txt/n\n",
            "2\n",
            "0\n",
            "1474 1949 698 528\n",
            "1\n",
            "1329 2637 2659 2572\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141636.txt/n\n",
            "1\n",
            "0\n",
            "1530 1702 868 761\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142554.txt/n\n",
            "1\n",
            "0\n",
            "1681 2327 685 501\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142828.txt/n\n",
            "1\n",
            "0\n",
            "1492 2087 835 656\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141859.txt/n\n",
            "1\n",
            "0\n",
            "2163 2173 773 640\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142414.txt/n\n",
            "2\n",
            "0\n",
            "698 1861 579 424\n",
            "1\n",
            "2013 2383 708 531\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_141829.txt/n\n",
            "1\n",
            "0\n",
            "1606 1395 816 812\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143009.txt/n\n",
            "1\n",
            "0\n",
            "1758 2031 630 726\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143333.txt/n\n",
            "1\n",
            "0\n",
            "1590 1914 838 659\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142049.txt/n\n",
            "1\n",
            "0\n",
            "1652 1873 847 719\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143158.txt/n\n",
            "1\n",
            "0\n",
            "1754 1506 866 762\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143243.txt/n\n",
            "1\n",
            "0\n",
            "1253 1692 824 760\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142117.txt/n\n",
            "1\n",
            "0\n",
            "1615 2439 822 725\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142930.txt/n\n",
            "1\n",
            "0\n",
            "1451 840 815 886\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_143020.txt/n\n",
            "1\n",
            "0\n",
            "1965 1698 462 785\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142136.txt/n\n",
            "1\n",
            "0\n",
            "1507 2009 687 567\n",
            "/content/drive/MyDrive/Last/data/IMG_20240402_142815.txt/n\n",
            "1\n",
            "0\n",
            "1603 1753 725 659\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "folder_path = save_folder = \"/content/drive/MyDrive/Last/data\"\n",
        "\n",
        "# function for creating bounding box just below the image\n",
        "def create_bounding_box(tag_bbox):\n",
        "\n",
        "    x, y, w, h = tag_bbox # x and y are coordinates of center\n",
        "    new_x = x\n",
        "    new_y = y + int(h/2) + 250\n",
        "    new_w = 500    # i made 200x200 bounding box just below the tag, increased size to 400x400\n",
        "    new_h = 500\n",
        "\n",
        "    # print(x, y, w, h)\n",
        "    return (new_x, new_y, new_w, new_h)\n",
        "\n",
        "# function for normalilzing the coordinates for yolov8\n",
        "def normal(new_bbox):\n",
        "    x, y, w, h = new_bbox\n",
        "    n_x = x/width\n",
        "    n_y = y/height\n",
        "    n_w = w/width\n",
        "    n_h = h/height\n",
        "    return (n_x, n_y, n_w, n_h)\n",
        "\n",
        "\n",
        "for j in range(len(n)):\n",
        "\n",
        "\n",
        "    tag_bbox = n[j][\"bbox\"]  # array of bounding box in the image\n",
        "    Image_nam = n[j]['image_name']  # name of the image\n",
        "    filenam = Image_nam + \".txt\"    # name of txt file\n",
        "\n",
        "    filenam = os.path.join(folder_path, filenam)\n",
        "\n",
        "    # create a empty txt file for a particular image with name -> filenam = Image_nam + \".txt\"\n",
        "    with open(filenam, 'w') as file:\n",
        "        # print(filenam + \"/n\")\n",
        "        pass\n",
        "\n",
        "    print(len(tag_bbox))\n",
        "    for i in range(len(tag_bbox)):\n",
        "        # print(i)\n",
        "        old_bbox = tag_bbox[i]\n",
        "        new_bbox = create_bounding_box(old_bbox)   # creating new bounding box below the tag\n",
        "\n",
        "        new_x, new_y, new_w, new_h = new_bbox\n",
        "\n",
        "        #  condition if the bounding box is going beyond the edges # 4000 and 3000 is the width and height of the images(in my data set)\n",
        "        # it is hard coded as of now, we can write code for it in future\n",
        "        if new_x < 0 or new_x + 500 > width or new_y < 0 or new_y + 500 > height :\n",
        "           continue\n",
        "        else :\n",
        "          n_x, n_y, n_w, n_h = normal(new_bbox)   # normalized values of bbox\n",
        "\n",
        "          with open(filenam, 'a') as file:\n",
        "              file.write(f\"0 {n_x} {n_y} {n_w} {n_h} \\n\")  # appending in txt files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCEbum3p6ugL"
      },
      "source": [
        "# main Augment - run krna hai ise\n",
        "\n",
        "Albumentation : Albumentations is a powerful open-source image augmentation library.\n",
        "using Albumentation library for different augmentation :\n",
        "1. Vertical flip\n",
        "2. Defocus\n",
        "3. Gray\n",
        "4. Scale\n",
        "5. Rotate\n",
        "7. Blur\n",
        "\n",
        "\n",
        "https://albumentations.ai/docs/getting_started/transforms_and_targets/\n",
        "\n",
        "\n",
        "The augmentation was needed to be done coz too much similarity in the image was overfitting the model, also we have less number of images in over dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R31lcVi57Nve"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW35ehhxcFrz"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source and destination paths\n",
        "source_folder = '/content/drive/MyDrive/Last/data'\n",
        "destination_folder = '/content/drive/MyDrive/Last/data2'\n",
        "\n",
        "# List all files in the source folder\n",
        "files = os.listdir(source_folder)\n",
        "\n",
        "# Copy each file to the destination folder\n",
        "for file in files:\n",
        "    source_path = os.path.join(source_folder, file)\n",
        "    destination_path = os.path.join(destination_folder, file)\n",
        "    shutil.copy(source_path, destination_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vertical"
      ],
      "metadata": {
        "id": "eNS_kIh4pZbB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL3LQ856eaAj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def process_images_in_folder(folder_path, output_directory):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            base_image_path = os.path.join(folder_path, filename)\n",
        "            base_txt_path = os.path.splitext(base_image_path)[0] + \".txt\"\n",
        "            augment_image_and_points(base_image_path, base_txt_path, output_directory)\n",
        "\n",
        "\n",
        "def augment_image_and_points(image_path, txt_path, output_dir):\n",
        "    image = cv2.imread(image_path)\n",
        "    global img_width, img_height\n",
        "    img_height, img_width, _ = image.shape\n",
        "    annotations = read_points_from_txt(txt_path)\n",
        "    filename = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    # Scale(annotations, image, 0.5, output_dir, filename)\n",
        "    Vertical(annotations, image, 0.7, output_dir, filename)\n",
        "\n",
        "\n",
        "def read_points_from_txt(txt_file):\n",
        "\n",
        "    # read lines from text file\n",
        "    with open(txt_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for line in lines:\n",
        "        # read data from each line\n",
        "        data = line.strip().split()\n",
        "        class_id, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        class_id = int(class_id)\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates in coco format/ openCV format\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        box = x1, y1, width, height\n",
        "        annotations.append((class_id, box))\n",
        "        return annotations\n",
        "\n",
        "def Vertical(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.VerticalFlip(p=0.5)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"vertical_flip \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    # plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"vertical_flip {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Last/data\"\n",
        "output_directory = \"/content/drive/MyDrive/Last/data2\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "process_images_in_folder(folder_path, output_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defocus\n"
      ],
      "metadata": {
        "id": "H57poaDeOUwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def process_images_in_folder(folder_path, output_directory):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            base_image_path = os.path.join(folder_path, filename)\n",
        "            base_txt_path = os.path.splitext(base_image_path)[0] + \".txt\"\n",
        "            augment_image_and_points(base_image_path, base_txt_path, output_directory)\n",
        "\n",
        "\n",
        "def augment_image_and_points(image_path, txt_path, output_dir):\n",
        "    image = cv2.imread(image_path)\n",
        "    global img_width, img_height\n",
        "    img_height, img_width, _ = image.shape\n",
        "    annotations = read_points_from_txt(txt_path)\n",
        "    filename = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    # Scale(annotations, image, 0.5, output_dir, filename)\n",
        "    Vertical(annotations, image, 0.7, output_dir, filename)\n",
        "\n",
        "\n",
        "def read_points_from_txt(txt_file):\n",
        "\n",
        "    # read lines from text file\n",
        "    with open(txt_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for line in lines:\n",
        "        # read data from each line\n",
        "        data = line.strip().split()\n",
        "        class_id, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        class_id = int(class_id)\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates in coco format/ openCV format\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        box = x1, y1, width, height\n",
        "        annotations.append((class_id, box))\n",
        "        return annotations\n",
        "\n",
        "def Vertical(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.Defocus(radius=(3, 10), alias_blur=(0.1, 0.5), always_apply=True, p=0.5)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"Defocus \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    # plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"Defocus {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Last/data\"\n",
        "output_directory = \"/content/drive/MyDrive/Last/data2\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "process_images_in_folder(folder_path, output_directory)\n"
      ],
      "metadata": {
        "id": "6prmtxW7Mcx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gray"
      ],
      "metadata": {
        "id": "VOOLlpIAtlVI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCcFrhCnrxJy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "# Path to the folder containing images and text files\n",
        "folder_path = '/content/drive/MyDrive/Last/data'\n",
        "\n",
        "# Create a new folder to save the grayscale images\n",
        "output_folder = '/content/drive/MyDrive/Last/data2'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.jpg'):  # Assuming images are in JPG format\n",
        "        # Load the image\n",
        "        image_path = os.path.join(folder_path, file_name)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            # Convert the image to grayscale\n",
        "            gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "            # Save the grayscale image\n",
        "            new_image_name = file_name.split('.')[0] + '_gray.jpg'\n",
        "            new_image_path = os.path.join(output_folder, new_image_name)\n",
        "            cv2.imwrite(new_image_path, gray_image)\n",
        "\n",
        "            # Copy and rename the corresponding text file\n",
        "            txt_file_name = file_name.split('.')[0] + '.txt'\n",
        "            txt_file_path = os.path.join(folder_path, txt_file_name)\n",
        "            new_txt_file_path = os.path.join(output_folder, new_image_name.split('.')[0] + '.txt')\n",
        "            shutil.copy(txt_file_path, new_txt_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale"
      ],
      "metadata": {
        "id": "g8Lv2RlRpVIx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEJ5Wx8ClwAe"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def process_images_in_folder(folder_path, output_directory):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            base_image_path = os.path.join(folder_path, filename)\n",
        "            base_txt_path = os.path.splitext(base_image_path)[0] + \".txt\"\n",
        "            augment_image_and_points(base_image_path, base_txt_path, output_directory)\n",
        "\n",
        "\n",
        "def augment_image_and_points(image_path, txt_path, output_dir):\n",
        "    image = cv2.imread(image_path)\n",
        "    global img_width, img_height\n",
        "    img_height, img_width, _ = image.shape\n",
        "    annotations = read_points_from_txt(txt_path)\n",
        "    filename = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    # Scale(annotations, image, 0.5, output_dir, filename)\n",
        "    Scale(annotations, image, 0.2, output_dir, filename)\n",
        "    # Vertical(annotations, image, 0.7, output_dir, filename)\n",
        "\n",
        "\n",
        "def read_points_from_txt(txt_file):\n",
        "\n",
        "    # read lines from text file\n",
        "    with open(txt_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for line in lines:\n",
        "        # read data from each line\n",
        "        data = line.strip().split()\n",
        "        class_id, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        class_id = int(class_id)\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates in coco format/ openCV format\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        box = x1, y1, width, height\n",
        "        annotations.append((class_id, box))\n",
        "        return annotations\n",
        "\n",
        "def Scale(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.RandomSizedBBoxSafeCrop(width=img_width, height=img_height, erosion_rate=rate)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"Scale \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    # plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"Scale {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Last/data\"\n",
        "output_directory = \"/content/drive/MyDrive/Last/data2\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "process_images_in_folder(folder_path, output_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rotate**"
      ],
      "metadata": {
        "id": "h_DVQ2uMcoIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# function for the\n",
        "def process_images_in_folder(folder_path, output_directory):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            base_image_path = os.path.join(folder_path, filename)\n",
        "            base_txt_path = os.path.splitext(base_image_path)[0] + \".txt\"\n",
        "            augment_image_and_points(base_image_path, base_txt_path, output_directory)\n",
        "\n",
        "\n",
        "def augment_image_and_points(image_path, txt_path, output_dir):\n",
        "    image = cv2.imread(image_path)\n",
        "    global img_width, img_height\n",
        "    img_height, img_width, _ = image.shape\n",
        "    annotations = read_points_from_txt(txt_path)\n",
        "    filename = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    # Scale(annotations, image, 0.5, output_dir, filename)\n",
        "    SafeRotate(annotations, image, 0.2, output_dir, filename)\n",
        "    # Vertical(annotations, image, 0.7, output_dir, filename)\n",
        "\n",
        "\n",
        "def read_points_from_txt(txt_file):\n",
        "\n",
        "    # read lines from text file\n",
        "    with open(txt_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for line in lines:\n",
        "        # read data from each line\n",
        "        data = line.strip().split()\n",
        "        class_id, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        class_id = int(class_id)\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates in coco format/ openCV format\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        box = x1, y1, width, height\n",
        "        annotations.append((class_id, box))\n",
        "        return annotations\n",
        "\n",
        "def SafeRotate(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.SafeRotate(limit=90, p=0.5)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"Rotate \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    # plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"Rotate {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "\n",
        "# start of code\n",
        "folder_path = \"/content/drive/MyDrive/Last/data\"\n",
        "output_directory = \"/content/drive/MyDrive/Last/data2\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "process_images_in_folder(folder_path, output_directory)\n"
      ],
      "metadata": {
        "id": "yCPdqf9IpF8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blur"
      ],
      "metadata": {
        "id": "w0P_x98UEZjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def process_images_in_folder(folder_path, output_directory):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            base_image_path = os.path.join(folder_path, filename)\n",
        "            base_txt_path = os.path.splitext(base_image_path)[0] + \".txt\"\n",
        "            augment_image_and_points(base_image_path, base_txt_path, output_directory)\n",
        "\n",
        "\n",
        "def augment_image_and_points(image_path, txt_path, output_dir):\n",
        "    image = cv2.imread(image_path)\n",
        "    global img_width, img_height\n",
        "    img_height, img_width, _ = image.shape\n",
        "    annotations = read_points_from_txt(txt_path)\n",
        "    filename = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "    # Scale(annotations, image, 0.5, output_dir, filename)\n",
        "    SafeRotate(annotations, image, 0.2, output_dir, filename)\n",
        "    # Vertical(annotations, image, 0.7, output_dir, filename)\n",
        "\n",
        "\n",
        "def read_points_from_txt(txt_file):\n",
        "\n",
        "    # read lines from text file\n",
        "    with open(txt_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for line in lines:\n",
        "        # read data from each line\n",
        "        data = line.strip().split()\n",
        "        class_id, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        class_id = int(class_id)\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates in coco format/ openCV format\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        box = x1, y1, width, height\n",
        "        annotations.append((class_id, box))\n",
        "        return annotations\n",
        "\n",
        "def SafeRotate(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        # [A.ToGray(p=0.5)],\n",
        "        [A.Blur(blur_limit=(3, 7), always_apply=True, p=0.5)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"BlurA \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    # plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"BlurA {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Last/data\"\n",
        "output_directory = \"/content/drive/MyDrive/Last/data2\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "process_images_in_folder(folder_path, output_directory)\n"
      ],
      "metadata": {
        "id": "feaUBdeIBYco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (no need to run) full augment in one go, but RAM insuffiecient\n",
        "In colab, we have RAM usage limit, coz of that we were unable to perform all augmentation in a single cell"
      ],
      "metadata": {
        "id": "ywGj7OnbklNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwekJkUe2pnp"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "def process_images_in_folder(folder_path, output_directory):\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
        "            base_image_path = os.path.join(folder_path, filename)\n",
        "            base_txt_path = os.path.splitext(base_image_path)[0] + \".txt\"\n",
        "            augment_image_and_points(base_image_path, base_txt_path, output_directory)\n",
        "\n",
        "\n",
        "def augment_image_and_points(image_path, txt_path, output_dir):\n",
        "    image = cv2.imread(image_path)\n",
        "    global img_width, img_height\n",
        "    img_height, img_width, _ = image.shape\n",
        "    annotations = read_points_from_txt(txt_path)\n",
        "    filename = os.path.basename(image_path)\n",
        "    name, ext = os.path.splitext(filename)\n",
        "\n",
        "\n",
        "    # Scale(annotations, image, 1, output_dir, filename)\n",
        "    # Scale(annotations, image, 1.25, output_dir, filename)\n",
        "    Scale(annotations, image, 0.5, output_dir, filename)\n",
        "    # Scale(annotations, image, 0.8, output_dir, filename)\n",
        "    Vertical(annotations, image, 0.7, output_dir, filename)\n",
        "    # Horizontal(annotations, image, 0.7, output_dir, filename)\n",
        "    # operations = [\n",
        "    #     (\"original\", image, annotations),\n",
        "    #     (\"flip_horizontal\", cv2.flip(image, 1), [(cls_id, horizontal(points, image_width)) for cls_id, points in annotations]),\n",
        "    #     (\"flip_vertical\", cv2.flip(image, 0), [(cls_id, vertical(points, image_height)) for cls_id, points in annotations])\n",
        "    # ]\n",
        "\n",
        "    # for operation, img, ann in operations:\n",
        "    #     output_image_path = os.path.join(output_dir, f\"{name}_{operation}.jpg\")\n",
        "    #     output_txt_path = os.path.join(output_dir, f\"{name}_{operation}.txt\")\n",
        "    #     cv2.imwrite(output_image_path, img)\n",
        "    #     write_points_to_txt(output_txt_path, ann, image_width, image_height)\n",
        "\n",
        "\n",
        "def write_points_to_txt(txt_file, annotations, image_width, image_height):\n",
        "    with open(txt_file, 'w') as f:\n",
        "        for cls_id, box in annotations:\n",
        "            x, y, w, h = box\n",
        "            cx =  (x + w/2) / image_width\n",
        "            cy = (y + h/2) / image_height\n",
        "            w = w / image_width\n",
        "            h = h / image_height\n",
        "            f.write(f\"{cls_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "def read_points_from_txt(txt_file):\n",
        "\n",
        "    # read lines from text file\n",
        "    with open(txt_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    for line in lines:\n",
        "        # read data from each line\n",
        "        data = line.strip().split()\n",
        "        class_id, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        class_id = int(class_id)\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates in coco format/ openCV format\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        box = x1, y1, width, height\n",
        "        annotations.append((class_id, box))\n",
        "        return annotations\n",
        "\n",
        "\n",
        "def Horizontal(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.HorizontalFlip(p=0.5)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"horizontal_flip \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"horizontal_flip {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def Scale(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.RandomSizedBBoxSafeCrop(width=img_width, height=img_height, erosion_rate=rate)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"Scale \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"Scale {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def Vertical(annotations, image, rate, output_dir, filename):\n",
        "    bboxes = []\n",
        "    category_ids = []\n",
        "    for class_id, box in annotations :\n",
        "      b = []\n",
        "      x, y, w, h = box\n",
        "      b.append(x)\n",
        "      b.append(y)\n",
        "      b.append(w)\n",
        "      b.append(h)\n",
        "      bboxes.append(b)\n",
        "      category_ids.append(class_id)\n",
        "\n",
        "\n",
        "    # # We will use the mapping from category_id to the class name\n",
        "    # # to visualize the class label for the bounding box on the image\n",
        "    category_id_to_name = {0: 'sharpener'}\n",
        "\n",
        "    # augmentation - vertical flip\n",
        "    transform = A.Compose(\n",
        "        [A.VerticalFlip(p=0.5)],\n",
        "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
        "    )\n",
        "    random.seed(7)\n",
        "    transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids)\n",
        "\n",
        "    # saving image\n",
        "    img = transformed['image'].copy()\n",
        "    filename_v = \"vertical_flip \"+ str(rate) + filename\n",
        "    output_image_path = os.path.join(output_dir, filename_v)\n",
        "    plt.imshow(img)\n",
        "    cv2.imwrite(output_image_path, img)\n",
        "\n",
        "\n",
        "    # creating text file\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    output_txt_path = os.path.join(output_dir, f\"vertical_flip {rate}{name}.txt\")\n",
        "\n",
        "    with open(output_txt_path, 'w') as f:\n",
        "      for j in range(len(transformed['bboxes'])):\n",
        "            x, y, w, h =   transformed['bboxes'][j]\n",
        "            cx =  (x + w/2) / img_width\n",
        "            cy = (y + h/2) / img_height\n",
        "            w = w / img_width\n",
        "            h = h / img_height\n",
        "            class_id = transformed[\"category_ids\"][j]\n",
        "            f.write(f\"{class_id} {cx} {cy} {w} {h}\\n\")\n",
        "\n",
        "\n",
        "# hello = horizontal(box, image, img_height, img_width, 2)\n",
        "# hello = horizontal(box, image, img_height, img_width, 0.2)\n",
        "# hello = horizontal(box, image, img_height, img_width, 0.7)\n",
        "# hello = horizontal(box, image, img_height, img_width, 1)\n",
        "# hello = horizontal(box, image, img_height, img_width, 1.5)\n",
        "# print(hello)\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/final/object\"\n",
        "output_directory = \"/content/drive/MyDrive/final/augment\"\n",
        "\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "process_images_in_folder(folder_path, output_directory)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdKecROsfM10"
      },
      "source": [
        "# *Visualization - No need to run it*\n",
        "\n",
        "for visualization, I used openCV\n",
        "\n",
        "fistly, I converted the normalized coordinated from txt to the coordinate form of openCV and then plotted a rectangle around object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBFg6dqXh0MK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Path to the folder containing images and YOLOv8 txt files\n",
        "folder_path = '/content/drive/MyDrive/Last/gray'\n",
        "\n",
        "# Iterate over each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.jpg'):  # Assuming images are JPEG format\n",
        "        # Load the image\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        image = cv2.imread(img_path)\n",
        "\n",
        "        # Load the corresponding YOLOv8 txt file\n",
        "        txt_filename = os.path.splitext(filename)[0] + '.txt'\n",
        "        txt_path = os.path.join(folder_path, txt_filename)\n",
        "\n",
        "        # Read the bounding box coordinates from the YOLOv8 txt file\n",
        "        with open(txt_path, 'r') as f:\n",
        "            data = f.readline().strip().split()\n",
        "            class_name, x_center, y_center, width, height = map(float, data)\n",
        "\n",
        "        # Convert YOLO format to OpenCV format\n",
        "        img_height, img_width, _ = image.shape\n",
        "        x_center = int(x_center * img_width)\n",
        "        y_center = int(y_center * img_height)\n",
        "        width = int(width * img_width)\n",
        "        height = int(height * img_height)\n",
        "\n",
        "        # Calculate the bounding box coordinates\n",
        "        x1 = int(x_center - width / 2)\n",
        "        y1 = int(y_center - height / 2)\n",
        "        x2 = int(x_center + width / 2)\n",
        "        y2 = int(y_center + height / 2)\n",
        "\n",
        "        # Draw the bounding box on the image\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "\n",
        "        # Display the image with the bounding box\n",
        "        # cv2.imshow('Image', image)\n",
        "        cv2.imwrite('/content/drive/MyDrive/Last/visual/' + os.path.splitext(filename)[0] + \" copy.jpg\", image)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn6E2aX55jDD"
      },
      "source": [
        "# Split dataset\n",
        "\n",
        "Spliting the dataset into 3 folders(8:1:1) namely :\n",
        "1. train\n",
        "2. val\n",
        "3. test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzxOu6Y55im4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define the path to your dataset directory\n",
        "dataset_dir = '/content/drive/MyDrive/Last/data2'\n",
        "\n",
        "# Define the output directories for train, validation, and test sets\n",
        "train_dir = '/content/drive/MyDrive/Last/self/dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/Last/self/dataset/val'\n",
        "test_dir = '/content/drive/MyDrive/Last/self/dataset/test'\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Create the output directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Get the list of images in the dataset directory\n",
        "images = [f for f in os.listdir(dataset_dir) if f.endswith('.jpg')]\n",
        "\n",
        "# Shuffle the images\n",
        "random.shuffle(images)\n",
        "\n",
        "# Calculate the number of images for each set\n",
        "num_images = len(images)\n",
        "num_train = int(num_images * train_ratio)\n",
        "num_val = int(num_images * val_ratio)\n",
        "num_test = num_images - num_train - num_val\n",
        "\n",
        "# Split the dataset\n",
        "train_images = images[:num_train]\n",
        "val_images = images[num_train:num_train + num_val]\n",
        "test_images = images[num_train + num_val:]\n",
        "\n",
        "# Copy the images and annotation files to the respective directories\n",
        "for image in train_images:\n",
        "    shutil.copy(os.path.join(dataset_dir, image), os.path.join(train_dir, image))\n",
        "    shutil.copy(os.path.join(dataset_dir, image.replace('.jpg', '.txt')), os.path.join(train_dir, image.replace('.jpg', '.txt')))\n",
        "\n",
        "for image in val_images:\n",
        "    shutil.copy(os.path.join(dataset_dir, image), os.path.join(val_dir, image))\n",
        "    shutil.copy(os.path.join(dataset_dir, image.replace('.jpg', '.txt')), os.path.join(val_dir, image.replace('.jpg', '.txt')))\n",
        "\n",
        "for image in test_images:\n",
        "    shutil.copy(os.path.join(dataset_dir, image), os.path.join(test_dir, image))\n",
        "    shutil.copy(os.path.join(dataset_dir, image.replace('.jpg', '.txt')), os.path.join(test_dir, image.replace('.jpg', '.txt')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFg_DbXhNRoe"
      },
      "source": [
        "# Final model run\n",
        "Training the model(used yolov8s.pt as pretrained model) on different number of epochs and batch size and other hyperparameters\n",
        "\n",
        "best results on :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "%cd /content/drive/MyDrive/Last/self\n",
        "model = YOLO('yolov8s.pt')\n",
        "model.train(data = \"/content/drive/MyDrive/Last/self/data.yaml\", epochs = 20, batch=8, imgsz=1280, verbose=True, single_cls=True, dropout=0.1, save_conf=True, plots=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "senVixJBllzg",
        "outputId": "236e83d6-006a-4aa6-8c45-6c581ec72ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/Last/self'\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to yolov8s.pt...\n",
            "ERROR: Downloaded file 'yolov8s.pt' does not exist or size is < min_bytes=100000.0\n",
            "yolov8s.pt missing, try downloading from https://github.com/ultralytics/assets/releases/v0.0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-17608807adad>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Last/self'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov8s.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Last/self/data.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, type)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;34m{\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.yaml'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{suffix}' model loading not implemented\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;31m# Loads a single model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloads\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattempt_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# search online if missing locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/utils/downloads.py\u001b[0m in \u001b[0;36mattempt_download\u001b[0;34m(file, repo, release)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make parent dir (if required)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             safe_download(file,\n\u001b[0m\u001b[1;32m     95\u001b[0m                           \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'https://github.com/{repo}/releases/download/{tag}/{name}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                           \u001b[0mmin_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1E5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/utils/downloads.py\u001b[0m in \u001b[0;36msafe_download\u001b[0;34m(file, url, url2, min_bytes, error_msg)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# url1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Downloading {url} to {file}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massert_msg\u001b[0m  \u001b[0;31m# check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# url2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mfile_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"torch.hub\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getheaders'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentry_sdk/integrations/stdlib.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreal_getresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_getresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_http_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "%cd /content/drive/MyDrive/final/self_model_tag_removed\n",
        "model = YOLO('yolov8s.pt')\n",
        "model.train(data = \"/content/drive/MyDrive/final/self_model_tag_removed/data.yaml\", epochs = 20, batch=8, imgsz=1280, verbose=True, single_cls=True, dropout=0.1, save_conf=True, plots=True)"
      ],
      "metadata": {
        "id": "zMEiL2Qm7m6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_zKqwQ5NWji"
      },
      "source": [
        "# testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHo2V8YtDfEX",
        "outputId": "372186b8-ac09-4b83-8367-44f4300e49f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-03 12:01:34.545814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-03 12:01:34.545959: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-03 12:01:34.550112: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-03 12:01:36.794852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "image 1/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.02 PM (1).jpeg: 1280x960 3050.1ms\n",
            "image 2/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.02 PM (2).jpeg: 1280x960 2115.8ms\n",
            "image 3/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.02 PM.jpeg: 1280x960 2092.7ms\n",
            "image 4/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.03 PM (1).jpeg: 1280x960 2170.5ms\n",
            "image 5/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.03 PM (2).jpeg: 1280x960 2242.4ms\n",
            "image 6/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.03 PM.jpeg: 1280x960 1 object, 3114.5ms\n",
            "image 7/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.19.04 PM.jpeg: 1280x960 2495.7ms\n",
            "image 8/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.29.10 PM.jpeg: 1280x736 1629.8ms\n",
            "image 9/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.29.43 PM.jpeg: 1280x736 1 object, 1663.7ms\n",
            "image 10/10 /content/drive/MyDrive/overfitting/self_learn/testing3/WhatsApp Image 2024-04-03 at 5.29.58 PM.jpeg: 1280x736 1 object, 1603.6ms\n",
            "Speed: 12.7ms pre-process, 2217.9ms inference, 1.2ms postprocess per image at shape (1, 3, 1280, 1280)\n",
            "Results saved to \u001b[1mruns/detect/predict10\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/overfitting/self_learn\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "!yolo task=detect mode=predict model=\"/content/drive/MyDrive/overfitting/self_learn/runs/detect/train10/weights/best.pt\" conf=0.25 source=\"/content/drive/MyDrive/overfitting/self_learn/testing3\" save=true plots=true"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/final/self_model\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "!yolo val model=\"/content/drive/MyDrive/final/self_model/runs/detect/train2/weights/best.pt\" data=\"/content/drive/MyDrive/final/self_model/data.yaml\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFGkdXXVeA38",
        "outputId": "ae709b13-3156-48fd-da8d-ce311f2877e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/final/self_model\n",
            "2024-04-20 09:43:35.314469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-20 09:43:35.314533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-20 09:43:35.316409: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-20 09:43:37.261573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "YOLOv8n summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/final/self_model/dataset/val.cache... 31 images, 0 backgrounds, 0 corrupt: 100% 31/31 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 2/2 [00:20<00:00, 10.50s/it]\n",
            "                   all         31         31      0.935      0.924      0.974      0.628\n",
            "Speed: 8.8ms pre-process, 236.2ms inference, 0.0ms loss, 1.0ms post-process per image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVwaQ6MuaYWf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"/content/drive/MyDrive/final/self_model/runs/detect/train2/weights/best.pt\")\n",
        "\n",
        "\n",
        "# metrics = model.val(data=\"/content/drive/MyDrive/final/self_model/data.yaml\", plots=True, imgsz=640,\n",
        "#                                batch=16,\n",
        "#                                conf=0.25,\n",
        "#                                iou=0.6)\n",
        "\n",
        "print(model.val(data=\"/content/drive/MyDrive/final/self_model/data.yaml\", plots=True,\n",
        "                              #  imgsz=640,\n",
        "                              #  batch=16,\n",
        "                               conf=0.25,\n",
        "                               iou=0.6))\n",
        "# print(metrics)\n",
        "# metrics.box.map\n",
        "# metrics.box.map50\n",
        "# metrics.box.map75\n",
        "# metrics.box.maps\n"
      ],
      "metadata": {
        "id": "SBp-VltosOG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a43894-00ad-4b8d-fa3d-64ad97a507e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU\n",
            "YOLOv8n summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/final/self_model/dataset/val.cache... 31 images, 0 backgrounds, 0 corrupt: 100%|██████████| 31/31 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:22<00:00, 11.26s/it]\n",
            "                   all         31         31          1      0.935      0.968      0.637\n",
            "Speed: 5.7ms pre-process, 227.0ms inference, 0.0ms loss, 1.3ms post-process per image\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow==2.13.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZa-HIF3kTwS",
        "outputId": "9a048d34-9373-4357-b1d7-b2716ea96585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.13.1 in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (1.62.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (3.9.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (2.13.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (18.1.1)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (1.24.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.1) (0.36.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.1) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "%cd /content/drive/MyDrive/final\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "\n",
        "\n",
        "model.export(format='tflite')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "qRbL17ZGf7Ea",
        "outputId": "a7f832b0-2ab9-456b-894e-cd3ce5e5f0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/final\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21.5M/21.5M [00:00<00:00, 43.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.2 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8s summary (fused): 168 layers, 11156544 parameters, 0 gradients, 28.6 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (21.5 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.13.1...\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.4.36...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 5.1s, saved as 'yolov8s.onnx' (42.8 MB)\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.17.5...\n",
            "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ✅ 50.2s, saved as 'yolov8s_saved_model' (107.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.13.1...\n",
            "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ✅ 0.0s, saved as 'yolov8s_saved_model/yolov8s_float32.tflite' (42.8 MB)\n",
            "\n",
            "Export complete (55.1s)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/final\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolov8s_saved_model/yolov8s_float32.tflite imgsz=640  \n",
            "Validate:        yolo val task=detect model=yolov8s_saved_model/yolov8s_float32.tflite imgsz=640 data=coco.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'yolov8s_saved_model/yolov8s_float32.tflite'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "%cd /content/drive/MyDrive/final/self_model\n",
        "# Load a pretrained YOLOv8n model\n",
        "model = YOLO('/content/drive/MyDrive/final/self_model/runs/detect/train2/weights/best_saved_model/best_float32.tflite')\n",
        "\n",
        "# Run inference\n",
        "model.predict('/content/drive/MyDrive/final/self_model/dataset/test/IMG_20240322_144227.jpg', save=True, imgsz=640, conf=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76bE5A63gl2N",
        "outputId": "ed260130-5430-4e93-d752-842b01c274b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/final/self_model\n",
            "Loading /content/drive/MyDrive/final/self_model/runs/detect/train2/weights/best_saved_model/best_float32.tflite for TensorFlow Lite inference...\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/final/self_model/dataset/test/IMG_20240322_144227.jpg: 640x640 2 objects, 556.8ms\n",
            "Speed: 25.1ms preprocess, 556.8ms inference, 3690.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict4\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ultralytics.engine.results.Results object with attributes:\n",
              " \n",
              " boxes: ultralytics.engine.results.Boxes object\n",
              " keypoints: None\n",
              " masks: None\n",
              " names: {0: 'object'}\n",
              " obb: None\n",
              " orig_img: array([[[ 34,  22,  18],\n",
              "         [ 38,  26,  22],\n",
              "         [ 35,  23,  19],\n",
              "         ...,\n",
              "         [ 75,  83,  96],\n",
              "         [ 93,  99, 112],\n",
              "         [ 95, 101, 114]],\n",
              " \n",
              "        [[ 39,  27,  23],\n",
              "         [ 39,  27,  23],\n",
              "         [ 37,  25,  21],\n",
              "         ...,\n",
              "         [ 82,  90, 103],\n",
              "         [ 99, 105, 118],\n",
              "         [ 99, 105, 118]],\n",
              " \n",
              "        [[ 41,  26,  23],\n",
              "         [ 42,  27,  24],\n",
              "         [ 39,  24,  21],\n",
              "         ...,\n",
              "         [ 96, 104, 117],\n",
              "         [111, 119, 132],\n",
              "         [105, 113, 126]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[169, 169, 155],\n",
              "         [167, 167, 153],\n",
              "         [159, 159, 145],\n",
              "         ...,\n",
              "         [120, 141, 133],\n",
              "         [123, 145, 140],\n",
              "         [124, 146, 141]],\n",
              " \n",
              "        [[162, 163, 147],\n",
              "         [159, 160, 144],\n",
              "         [160, 160, 146],\n",
              "         ...,\n",
              "         [124, 145, 137],\n",
              "         [126, 148, 143],\n",
              "         [128, 150, 145]],\n",
              " \n",
              "        [[166, 167, 151],\n",
              "         [157, 158, 142],\n",
              "         [166, 166, 152],\n",
              "         ...,\n",
              "         [121, 142, 134],\n",
              "         [122, 144, 139],\n",
              "         [122, 144, 139]]], dtype=uint8)\n",
              " orig_shape: (4624, 3472)\n",
              " path: '/content/drive/MyDrive/final/self_model/dataset/test/IMG_20240322_144227.jpg'\n",
              " probs: None\n",
              " save_dir: 'runs/detect/predict4'\n",
              " speed: {'preprocess': 25.097370147705078, 'inference': 556.8127632141113, 'postprocess': 3690.089225769043}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PbKgP7gmL-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qlVi1arsjR2n"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}